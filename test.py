import torch\nimport matplotlib.pyplot as plt\nfrom diffusers import StableDiffusionPipeline\nimport random\nimport numpy as np\nfrom PIL import Image\nimport argparse\n\n# --- Set Random Seed for Reproducibility ---\ntorch.manual_seed(42)\nrandom.seed(42)\nnp.random.seed(42)\n\ndef plot_images_side_by_side(image1_pil, title1, image2_pil, title2):\n    \"\"\"Plots two PIL images side by side.\"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n    \n    axes[0].imshow(image1_pil)\n    axes[0].set_title(title1)\n    axes[0].axis(\'off\')\n    \n    axes[1].imshow(image2_pil)\n    axes[1].set_title(title2)\n    axes[1].axis(\'off\')\n    \n    plt.tight_layout()\n    plt.show()\n\n# --- Define Timestep Sampling Function (from your script) ---\ndef sample_timestep(max_steps=30):\n    # Ensure timestep is within the number of inference steps\n    # Original logic was 0-15 (70%) or 16-30 (30%)\n    # Adjust if max_steps is different, but keep relative distribution\n    if max_steps < 16:\n        return random.randint(0, max_steps -1) # Or some other logic for very few steps\n    \n    if random.random() < 0.7:\n        upper_bound_early = min(15, max_steps - 1)\n        return random.randint(0, upper_bound_early) \n    else:\n        lower_bound_late = min(16, max_steps -1) # Ensure lower bound is valid\n        if lower_bound_late >= max_steps: # If max_steps is small, e.g. 16\             lower_bound_late = max(0, max_steps -2) # Adjust to make sure range is valid\n        return random.randint(lower_bound_late, max_steps - 1)\n\n# --- Define Callback Class (from your script) ---\nclass ImageCaptureCallback:\n    def __init__(self, target_timestep):\n        self.target_timestep = target_timestep\n        self.intermediate_latents = None # Store latents\n        self.actual_step_captured = -1\n\n    def __call__(self, pipe, step, t, callback_kwargs):\n        # step is 0-indexed internally for callbacks by diffusers\n        # target_timestep from sample_timestep is also 0-indexed effectively\n        if step == self.target_timestep:\n            self.intermediate_latents = callback_kwargs[\"latents\"].detach().clone()\n            self.actual_step_captured = step # Store the actual 0-indexed step\n        return callback_kwargs\n\ndef generate_and_plot(prompt, num_inference_steps=30):\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    torch_dtype = torch.float16 if device == \"cuda\" else torch.float32\n\n    print(\"Loading Stable Diffusion v1.4 pipeline...\")\n    try:\n        pipe = StableDiffusionPipeline.from_pretrained(\n            \"CompVis/stable-diffusion-v1-4\",\n            torch_dtype=torch_dtype,\n        ).to(device)\n    except Exception as e:\n        print(f\"Error loading pipeline: {e}\")\n        print(\"Please ensure you have an internet connection and the necessary libraries installed.\")\n        return\n\n    # --- Sample Timestep for capturing intermediate image ---\n    # sample_timestep expects 0-indexed, num_inference_steps is the total count.\n    # The callback step is 0-indexed from 0 to num_inference_steps-1.\n    target_capture_step = sample_timestep(max_steps=num_inference_steps) \n    print(f\"Targeting capture at (0-indexed) step: {target_capture_step}\")\n\n    # --- Create Callback Instance ---\n    callback = ImageCaptureCallback(target_capture_step)\n\n    # --- Generate Image with Callback ---\n    print(f\"Generating image with prompt: \'{prompt}\' (Steps: {num_inference_steps})\")\n    try:\n        output = pipe(\n            prompt=prompt,\n            num_inference_steps=num_inference_steps,\n            callback_on_step_end=callback,\n            callback_on_step_end_tensor_inputs=[\"latents\"], # Pass latents to callback\n            return_dict=True\n        )\n        final_image_pil = output.images[0]\n        print(\"Image generation complete.\")\n    except Exception as e:\n        print(f\"Error during image generation: {e}\")\n        return\n\n    # --- Process Intermediate Image ---\n    intermediate_image_pil = None\n    if callback.intermediate_latents is not None:\n        print(f\"Intermediate latents captured at actual step: {callback.actual_step_captured}\")\n        try:\n            # Decode latents using VAE\n            with torch.no_grad():\n                # Ensure latents are on the correct device and dtype for VAE\n                latents_for_decode = callback.intermediate_latents.to(pipe.vae.device, dtype=pipe.vae.dtype)\n                decoded_image_tensor = pipe.vae.decode(latents_for_decode / pipe.vae.config.scaling_factor, return_dict=False)[0]\n            \n            # Post-process tensor to PIL image (similar to pipeline\'s internal processing)\n            # This is typically -1 to 1, needs to be 0 to 1, then 0 to 255\n            decoded_image_tensor = (decoded_image_tensor / 2 + 0.5).clamp(0, 1)\n            # Permute C, H, W to H, W, C for PIL\n            decoded_image_tensor = decoded_image_tensor.cpu().permute(1, 2, 0).float().numpy()\n            intermediate_image_pil = Image.fromarray((decoded_image_tensor * 255).round().astype(\"uint8\"))\n            print(\"Intermediate latent decoded successfully.\")\n        except Exception as e:\n            print(f\"Error decoding intermediate latents: {e}\")\n    else:\n        print(f\"Warning: Intermediate latents were not captured. Target step was {target_capture_step}.\")\n\n    # --- Plot Results ---\n    if final_image_pil and intermediate_image_pil:\n        plot_images_side_by_side(\n            intermediate_image_pil,\n            f\"Decoded Intermediate (Captured Step: {callback.actual_step_captured})\",\n            final_image_pil, \n            f\"Final Image (Prompt: \'{prompt}\')\"\n        )\n        # The concept of a specific numerical \"timestep\" (like from a scheduler) is tied to the scheduler's state.\n        # The callback provides the `t` (current scheduler timestep value) if needed, but `step` is the iteration count.\n        # For this script, printing the captured step (iteration) is more direct.\n        print(f\"Displayed intermediate image captured at generation step: {callback.actual_step_captured} (0-indexed)\")\n    elif final_image_pil:\n        print(\"Final image generated, but intermediate image was not processed or captured.\")\n        final_image_pil.show(title=\"Final Image Only\")\n    else:\n        print(\"Image generation failed or no images were produced.\")\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Test Stable Diffusion v1.4 intermediate latent decoding.\")\n    parser.add_argument(\"--prompt\", type=str, default=\"A renaissance painting of a robot cat\", help=\"Prompt for image generation.\")\n    parser.add_argument(\"--steps\", type=int, default=30, help=\"Number of inference steps.\")\n    \n    args = parser.parse_args()\n    generate_and_plot(prompt=args.prompt, num_inference_steps=args.steps)\n 