import torch\nimport matplotlib.pyplot as plt\nfrom diffusers import StableDiffusionPipeline\nimport random\nimport numpy as np\nfrom PIL import Image\nimport argparse\n\n# --- Set Random Seed for Reproducibility ---\ntorch.manual_seed(42)\nrandom.seed(42)\nnp.random.seed(42)\n\ndef plot_images_side_by_side(image1_pil, title1, image2_pil, title2):\n    \"\"\"Plots two PIL images side by side.\"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n    \n    axes[0].imshow(image1_pil)\n    axes[0].set_title(title1)\n    axes[0].axis(\'off\')\n    \n    axes[1].imshow(image2_pil)\n    axes[1].set_title(title2)\n    axes[1].axis(\'off\')\n    \n    plt.tight_layout()\n    plt.show()\n\n# --- Define Timestep Sampling Function (from your script) ---\ndef sample_timestep(max_steps=30):\n    if max_steps <= 0: 
        return 0 # Handle edge case for 0 or negative steps\n    # Ensure timestep is within the number of inference steps (0 to max_steps-1)\n    if max_steps < 16:\n        return random.randint(0, max_steps - 1)\n    \n    if random.random() < 0.7:\n        upper_bound_early = min(15, max_steps - 1)\n        return random.randint(0, upper_bound_early) \n    else:\n        # Ensure lower_bound_late is valid and less than max_steps-1\n        lower_bound_late = min(16, max_steps - 1)\n        # If max_steps is small (e.g. 16), lower_bound_late could be max_steps-1.\n        # We need a range for randint, so ensure upper bound is at least lower_bound_late.\n        upper_bound_late = max_steps - 1\n        if lower_bound_late > upper_bound_late: # Should not happen if max_steps >=16\n            lower_bound_late = upper_bound_late \n        return random.randint(lower_bound_late, upper_bound_late)\n\n# --- Define Callback Class (from your script) ---\nclass ImageCaptureCallback:\n    def __init__(self, target_timestep):\n        self.target_timestep = target_timestep\n        self.intermediate_latents = None # Store latents\n        self.actual_step_captured = -1\n\n    def __call__(self, pipe, step, t, callback_kwargs):\n        # step is 0-indexed internally for callbacks by diffusers\n        if step == self.target_timestep:\n            self.intermediate_latents = callback_kwargs[\"latents\"].detach().clone()\n            self.actual_step_captured = step # Store the actual 0-indexed step\n        return callback_kwargs\n\ndef generate_and_plot(prompt, num_inference_steps=30):\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    torch_dtype = torch.float16 if device == \"cuda\" else torch.float32\n\n    print(\"Loading Stable Diffusion v1.4 pipeline...\")\n    try:\n        pipe = StableDiffusionPipeline.from_pretrained(\n            \"CompVis/stable-diffusion-v1-4\",\n            torch_dtype=torch_dtype,\n        ).to(device)\n    except Exception as e:\n        print(f\"Error loading pipeline: {e}\")\n        print(\"Please ensure you have an internet connection and the necessary libraries installed.\")\n        return\n\n    target_capture_step = sample_timestep(max_steps=num_inference_steps) \n    print(f\"Targeting capture at (0-indexed) step: {target_capture_step}\")\n\n    callback = ImageCaptureCallback(target_capture_step)\n\n    print(f\"Generating image with prompt: '{prompt}' (Steps: {num_inference_steps})\")\n    try:\n        output = pipe(\n            prompt=prompt,\n            num_inference_steps=num_inference_steps,\n            callback_on_step_end=callback,\n            callback_on_step_end_tensor_inputs=[\"latents\"],\n            return_dict=True\n        )\n        final_image_pil = output.images[0]\n        print(\"Image generation complete.\")\n    except Exception as e:\n        print(f\"Error during image generation: {e}\")\n        return\n\n    intermediate_image_pil = None\n    if callback.intermediate_latents is not None:\n        print(f\"Intermediate latents captured at actual step: {callback.actual_step_captured}\")\n        try:\n            with torch.no_grad():\n                latents_for_decode = callback.intermediate_latents.to(pipe.vae.device, dtype=pipe.vae.dtype)\n                decoded_image_tensor = pipe.vae.decode(latents_for_decode / pipe.vae.config.scaling_factor, return_dict=False)[0]\n            decoded_image_tensor = (decoded_image_tensor / 2 + 0.5).clamp(0, 1)\n            decoded_image_tensor = decoded_image_tensor.cpu().permute(1, 2, 0).float().numpy()\n            intermediate_image_pil = Image.fromarray((decoded_image_tensor * 255).round().astype(\"uint8\"))\n            print(\"Intermediate latent decoded successfully.\")\n        except Exception as e:\n            print(f\"Error decoding intermediate latents: {e}\")\n    else:\n        print(f\"Warning: Intermediate latents were not captured. Target step was {target_capture_step}.\")
\n\n    if final_image_pil and intermediate_image_pil:\n        plot_images_side_by_side(\n            intermediate_image_pil,\n            f\"Decoded Intermediate (Captured Step: {callback.actual_step_captured})\",\n            final_image_pil, \n            f\"Final Image (Prompt: '{prompt}')\"\n        )\n        print(f\"Displayed intermediate image captured at generation step: {callback.actual_step_captured} (0-indexed)\")\n    elif final_image_pil:\n        print(\"Final image generated, but intermediate image was not processed or captured.\")\n        final_image_pil.show(title=\"Final Image Only\")\n    else:\n        print(\"Image generation failed or no images were produced.\")\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Test Stable Diffusion v1.4 intermediate latent decoding.\")\n    parser.add_argument(\"--prompt\", type=str, default=\"A renaissance painting of a robot cat\", help=\"Prompt for image generation.\")\n    parser.add_argument(\"--steps\", type=int, default=30, help=\"Number of inference steps.\")\n    \n    args = parser.parse_args()\n    generate_and_plot(prompt=args.prompt, num_inference_steps=args.steps)\n 